# -*- coding: utf-8 -*-
# ml/simple_ml.py - FIXED VERSION

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report
from data.data_processor import create_ml_features, prepare_train_test_split


class SimpleMLTrainer:
    """Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ML Ø³Ø§Ø¯Ù‡ Ùˆ ØµØ­ÛŒØ­"""

    def __init__(self):
        self.models = {}

    def train_simple_model(self, df, symbol):
        """Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø¯ÙˆÙ† Data Leakage"""

        print(f"\n{'='*60}")
        print(f"ğŸ¯ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ {symbol}")
        print(f"{'='*60}")

        # Ø§ÛŒØ¬Ø§Ø¯ features Ùˆ target (Ø¨Ø¯ÙˆÙ† leakage)
        df_features, feature_columns = create_ml_features(df)

        if len(df_features) < 200:
            print(f"âš ï¸ Ø¯Ø§Ø¯Ù‡ Ù†Ø§Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ {symbol}: {len(df_features)} Ù†Ù…ÙˆÙ†Ù‡")
            return None, None, None

        # Ø¨Ø±Ø±Ø³ÛŒ ØªØ¹Ø§Ø¯Ù„ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
        target_counts = df_features["target"].value_counts()
        total = len(df_features)

        print(f"\nğŸ“Š ØªÙˆØ²ÛŒØ¹ Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§:")
        for cls in [-1, 0, 1]:
            count = target_counts.get(cls, 0)
            pct = count / total * 100
            class_name = {-1: "Sell", 0: "Hold", 1: "Buy"}[cls]
            print(f"   {class_name}: {count} ({pct:.1f}%)")

        # Ø§Ú¯Ø± Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Buy/Sell Ø®ÛŒÙ„ÛŒ Ú©Ù… Ù‡Ø³ØªÙ†Ø¯ØŒ Ø§Ø² binary target Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        buy_count = target_counts.get(1, 0)
        sell_count = target_counts.get(-1, 0)

        if buy_count < 30 or sell_count < 30:
            print(f"âš ï¸ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Buy/Sell Ú©Ù… Ù‡Ø³ØªÙ†Ø¯. ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ binary...")
            # ÙÙ‚Ø· Buy (1) Ùˆ Not Buy (0) Ø±Ùˆ Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±
            df_features["target"] = (df_features["target"] == 1).astype(int)

        # Split Ø¨Ø¯ÙˆÙ† leakage
        X_train, X_test, y_train, y_test, scaler = prepare_train_test_split(
            df_features, feature_columns, test_size=0.3
        )

        # Ø¨Ø±Ø±Ø³ÛŒ ØªÙˆØ²ÛŒØ¹ Ø¯Ø± train/test
        print(f"\nğŸ“Š ØªÙˆØ²ÛŒØ¹ Train: {y_train.value_counts().to_dict()}")
        print(f"ğŸ“Š ØªÙˆØ²ÛŒØ¹ Test: {y_test.value_counts().to_dict()}")

        # Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡
        print(f"\nğŸ¤– Ø¢Ù…ÙˆØ²Ø´ Random Forest...")
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=8,
            min_samples_split=20,
            min_samples_leaf=10,
            max_features="sqrt",
            class_weight="balanced",
            random_state=42,
            n_jobs=-1,
        )

        # Ø¢Ù…ÙˆØ²Ø´
        model.fit(X_train, y_train)

        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        train_acc = accuracy_score(y_train, y_train_pred)
        test_acc = accuracy_score(y_test, y_test_pred)

        train_f1 = f1_score(y_train, y_train_pred, average="weighted")
        test_f1 = f1_score(y_test, y_test_pred, average="weighted")

        print(f"\nğŸ“ˆ Ù†ØªØ§ÛŒØ¬:")
        print(f"   Train Accuracy: {train_acc:.3f}")
        print(f"   Test Accuracy: {test_acc:.3f}")
        print(f"   Train F1: {train_f1:.3f}")
        print(f"   Test F1: {test_f1:.3f}")

        # Ø¨Ø±Ø±Ø³ÛŒ overfitting
        overfit_gap = train_acc - test_acc
        if overfit_gap > 0.15:
            print(f"âš ï¸ Ø§Ø­ØªÙ…Ø§Ù„ Overfitting: Gap = {overfit_gap:.3f}")

        # Ù†Ù…Ø§ÛŒØ´ Classification Report
        print(f"\nğŸ“Š Classification Report (Test):")
        print(classification_report(y_test, y_test_pred))

        # Feature Importance
        feature_importance = pd.DataFrame(
            {"feature": feature_columns, "importance": model.feature_importances_}
        ).sort_values("importance", ascending=False)

        print(f"\nğŸ” Top 5 Features:")
        for idx, row in feature_importance.head(5).iterrows():
            print(f"   {row['feature']}: {row['importance']:.4f}")

        # Ù‚Ø¨ÙˆÙ„ Ù…Ø¯Ù„ Ø§Ú¯Ø±:
        # 1. Test accuracy Ø¨Ù‡ØªØ± Ø§Ø² baseline (0.5 Ø¨Ø±Ø§ÛŒ binary, 0.33 Ø¨Ø±Ø§ÛŒ 3-class)
        # 2. Test F1 Ù…Ø¹Ù‚ÙˆÙ„ Ø¨Ø§Ø´Ù‡
        # 3. Overfitting Ø²ÛŒØ§Ø¯ Ù†Ø¨Ø§Ø´Ù‡

        baseline = 0.5 if len(y_test.unique()) == 2 else 0.33

        if test_acc > baseline + 0.05 and test_f1 > 0.4 and overfit_gap < 0.20:
            print(f"\nâœ… Ù…Ø¯Ù„ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ø§Ø³Øª")
            return model, scaler, feature_columns
        else:
            print(f"\nâŒ Ù…Ø¯Ù„ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ù†ÛŒØ³Øª:")
            if test_acc <= baseline + 0.05:
                print(
                    f"   - Test Accuracy Ù¾Ø§ÛŒÛŒÙ†: {test_acc:.3f} vs baseline {baseline:.3f}"
                )
            if test_f1 <= 0.4:
                print(f"   - Test F1 Ù¾Ø§ÛŒÛŒÙ†: {test_f1:.3f}")
            if overfit_gap >= 0.20:
                print(f"   - Overfitting Ø²ÛŒØ§Ø¯: {overfit_gap:.3f}")
            return None, None, None

    def train_all_models(self, symbols_data):
        """Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ symbols"""
        successful_models = 0

        for symbol, df in symbols_data.items():
            if len(df) > 300:
                model, scaler, features = self.train_simple_model(df, symbol)

                if model is not None:
                    self.models[symbol] = (model, scaler, features, "sklearn")
                    successful_models += 1
                    print(f"\nâœ… Ù…Ø¯Ù„ {symbol} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯\n")
                else:
                    print(f"\nâŒ Ù…Ø¯Ù„ {symbol} Ø±Ø¯ Ø´Ø¯\n")

        print(f"\n{'='*60}")
        print(f"ğŸ‰ Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„: {successful_models}/{len(symbols_data)} Ù…Ø¯Ù„ Ù…ÙˆÙÙ‚")
        print(f"{'='*60}\n")

        return self.models
