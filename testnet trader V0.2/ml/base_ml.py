# -*- coding: utf-8 -*-
# ml/base_ml.py

import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import shap
from data.data_processor import create_ml_features


class MLModelManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""

    def __init__(self):
        self.models = {}  # {symbol: (model, scaler, features, model_type)}

    def save_models(self, path="models/"):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ø§Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„Ù Ù…Ø¯Ù„"""
        import os

        os.makedirs(path, exist_ok=True)

        for symbol, (model, scaler, features, model_type) in self.models.items():
            symbol_filename = symbol.replace("/", "_")

            try:
                # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹
                if model_type == "keras":
                    model.save(f"{path}{symbol_filename}_model.h5")
                else:  # scikit-learn models
                    joblib.dump(model, f"{path}{symbol_filename}_model.pkl")

                # Ø°Ø®ÛŒØ±Ù‡ scaler Ùˆ features
                joblib.dump(scaler, f"{path}{symbol_filename}_scaler.pkl")
                joblib.dump(features, f"{path}{symbol_filename}_features.pkl")
                joblib.dump(model_type, f"{path}{symbol_filename}_type.pkl")

                print(f"ğŸ’¾ Ù…Ø¯Ù„ {symbol} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ (Ù†ÙˆØ¹: {model_type})")

            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ {symbol}: {e}")

        print(f"âœ… ØªÙ…Ø§Ù… Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¯Ø± {path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")

    def load_models(self, path="models/"):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡"""
        import os
        from tensorflow.keras.models import load_model

        if not os.path.exists(path):
            print("ğŸ“‚ Ù¾ÙˆØ´Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return False

        loaded_count = 0
        for filename in os.listdir(path):
            if filename.endswith("_model.h5") or filename.endswith("_model.pkl"):
                try:
                    if filename.endswith("_model.h5"):
                        symbol = filename.replace("_model.h5", "").replace("_", "/")
                        model_type = "keras"
                    else:
                        symbol = filename.replace("_model.pkl", "").replace("_", "/")
                        model_type = "sklearn"

                    model_path = f"{path}{filename}"
                    scaler_path = f'{path}{symbol.replace("/", "_")}_scaler.pkl'
                    features_path = f'{path}{symbol.replace("/", "_")}_features.pkl'
                    type_path = f'{path}{symbol.replace("/", "_")}_type.pkl'

                    if all(os.path.exists(p) for p in [scaler_path, features_path]):
                        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
                        if model_type == "keras":
                            model = load_model(model_path)
                        else:
                            model = joblib.load(model_path)

                        scaler = joblib.load(scaler_path)
                        features = joblib.load(features_path)

                        # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ù†ÙˆØ¹ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ØŒ Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª Ø§Ø² Ù¾Ø³ÙˆÙ†Ø¯ ÙØ§ÛŒÙ„
                        if os.path.exists(type_path):
                            model_type = joblib.load(type_path)

                        self.models[symbol] = (model, scaler, features, model_type)
                        loaded_count += 1
                        print(f"ğŸ“‚ Ù…Ø¯Ù„ {symbol} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ (Ù†ÙˆØ¹: {model_type})")
                    else:
                        print(f"âš ï¸ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ {symbol} ÛŒØ§ÙØª Ù†Ø´Ø¯")
                except Exception as e:
                    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ {filename}: {e}")

        if loaded_count == 0:
            print("âš ï¸ Ù‡ÛŒÚ† Ù…Ø¯Ù„ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯")
            return False
        else:
            print(f"âœ… ØªØ¹Ø¯Ø§Ø¯ {loaded_count} Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
            return True

    def get_model(self, symbol):
        """Ø¯Ø±ÛŒØ§ÙØª Ù…Ø¯Ù„ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ÛŒÚ© Ø§Ø±Ø²"""
        return self.models.get(symbol, (None, None, None, None))

    def train_models_for_symbols(self, symbols_data):
        """Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ø§Ø±Ø²Ù‡Ø§"""
        print("ğŸ¤– Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†...")
        successful_models = 0

        for symbol, df in symbols_data.items():
            if len(df) > 300:
                print(f"ğŸ“š Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ {symbol}...")
                try:
                    df.name = symbol
                    model, scaler, features, model_type = self.train_ml_model(
                        df, symbol
                    )
                    if model is not None:
                        self.models[symbol] = (model, scaler, features, model_type)
                        successful_models += 1
                        print(f"âœ… Ù…Ø¯Ù„ {symbol} Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯")
                    else:
                        print(f"âŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ {symbol} Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯")
                except Exception as e:
                    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ {symbol}: {e}")
            else:
                print(f"âš ï¸ Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ {symbol} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ (ÙÙ‚Ø· {len(df)} Ú©Ù†Ø¯Ù„)")

        print(
            f"ğŸ¯ Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯: {successful_models} Ø§Ø² {len(symbols_data)} Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù†Ø¯"
        )

    def train_ml_model(self, df, symbol):
        """Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ LSTM Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù†Ù…Ø§Ø¯"""
        try:
            df_features, feature_columns = create_ml_features(df)

            if len(df_features) < 100:
                print(f"âš ï¸ Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ {symbol} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
                return None, None, None, None

            X = df_features[feature_columns]
            y = df_features["target"]

            # Ø¨Ø±Ø±Ø³ÛŒ ØªØ¹Ø§Ø¯Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            target_counts = y.value_counts()
            print(f"ğŸ“Š ØªÙˆØ²ÛŒØ¹ ØªØ§Ø±Ú¯Øª {symbol}: {dict(target_counts)}")

            # Ø§Ú¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ø®ÛŒÙ„ÛŒ Ú©Ù… Ù‡Ø³ØªÙ†Ø¯ØŒ Ø¢Ø³ØªØ§Ù†Ù‡ Ø±Ø§ Ú©Ø§Ù‡Ø´ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
            if target_counts.get(1, 0) < len(y) * 0.15:
                print(f"âš ï¸ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ú©Ù… Ù‡Ø³ØªÙ†Ø¯. ØªÙ†Ø¸ÛŒÙ… Ø¢Ø³ØªØ§Ù†Ù‡...")
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡â€ŒØªØ±
                from sklearn.ensemble import RandomForestClassifier
                from sklearn.metrics import f1_score

                X_train, X_test, y_train, y_test = train_test_split(
                    X, y, test_size=0.3, random_state=42, shuffle=False
                )

                model = RandomForestClassifier(
                    n_estimators=50, max_depth=10, min_samples_split=10, random_state=42
                )

                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                test_f1 = f1_score(y_test, y_pred)

                print(f"ğŸ” Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡ {symbol}: F1 = {test_f1:.3f}")

                if test_f1 > 0.1:  # Ø¢Ø³ØªØ§Ù†Ù‡ Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡
                    return model, None, feature_columns, "sklearn"
                else:
                    return None, None, None, None

            # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.3, random_state=42, shuffle=False
            )

            if len(X_train) < 50:
                print(f"âš ï¸ Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ {symbol} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
                return None, None, None, None

            # Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            # SMOTE Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§Ø¨Ù„Ù‡ Ø¨Ø§ Ø¹Ø¯Ù… ØªØ¹Ø§Ø¯Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            smote = SMOTE(random_state=42)
            X_train_resampled, y_train_resampled = smote.fit_resample(
                X_train_scaled, y_train
            )

            # Ù…Ø¯Ù„ LSTM
            X_train_lstm = X_train_resampled.reshape(
                (X_train_resampled.shape[0], 1, X_train_resampled.shape[1])
            )
            X_test_lstm = X_test_scaled.reshape(
                (X_test_scaled.shape[0], 1, X_test_scaled.shape[1])
            )

            model = Sequential(
                [
                    LSTM(
                        32,
                        return_sequences=True,
                        input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]),
                    ),
                    Dropout(0.2),
                    LSTM(16, return_sequences=False),
                    Dropout(0.2),
                    Dense(8, activation="relu"),
                    Dense(1, activation="sigmoid"),
                ]
            )

            model.compile(
                optimizer=Adam(learning_rate=0.001),
                loss="binary_crossentropy",
                metrics=["accuracy"],
            )

            print(f"â³ Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ LSTM Ø¨Ø±Ø§ÛŒ {symbol}...")
            history = model.fit(
                X_train_lstm,
                y_train_resampled,
                epochs=30,
                batch_size=16,
                validation_data=(X_test_lstm, y_test),
                verbose=0,
            )

            # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„
            train_accuracy = history.history["accuracy"][-1]
            val_accuracy = history.history["val_accuracy"][-1]
            print(
                f"âœ… Ù…Ø¯Ù„ {symbol} Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯ - Ø¯Ù‚Øª Ø¢Ù…ÙˆØ²Ø´: {train_accuracy:.3f}, Ø¯Ù‚Øª Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ: {val_accuracy:.3f}"
            )

            return model, scaler, feature_columns, "keras"

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ {symbol}: {e}")
            return None, None, None, None


def predict_with_ml(model, scaler, feature_columns, df_current, model_type="keras"):
    """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ ML Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø§Ø±ÛŒ"""
    available_features = [f for f in feature_columns if f in df_current.columns]

    if len(available_features) != len(feature_columns):
        missing_features = [f for f in feature_columns if f not in df_current.columns]
        print(f"âš ï¸ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØºØ§ÛŒØ¨: {len(missing_features)} Ø§Ø² {len(feature_columns)}")
        return 0.5

    X_current = df_current[available_features].iloc[[-1]]

    if scaler is not None:
        X_scaled = scaler.transform(X_current)
    else:
        X_scaled = X_current.values

    if model_type == "keras":
        X_lstm = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))
        probability = model.predict(X_lstm)[0][0]
    else:  # sklearn models
        probability = model.predict_proba(X_scaled)[0][1]

    return probability
